# CIPPROJECT - Disaster Tweet Classification System

A comprehensive disaster-related tweet classification system with a fine-tuned transformer model, explainable AI (XAI), and actionable information extraction. The system includes a MERN stack web application and a Python desktop dashboard for local model inference.

## ğŸ—ï¸ Architecture

The system consists of three main components:

1. **Backend (Node.js/Express/MongoDB)** - Stores tweets in database
2. **Frontend (React)** - Web interface for viewing tweets (optional)
3. **Desktop Dashboard (Python)** - **Runs the AI model locally** for classification with XAI visualization

**Key Features**: 
- The AI model **only runs locally** in the desktop dashboard - ensuring privacy and allowing offline operation
- **No Twitter API needed** in dashboard - works with database tweets only
- **Manual tweet input** - Enter tweets directly for classification
- **Database integration** - Load and classify tweets from MongoDB

## ğŸ“ Project Structure

```
CIPPROJECT/
â”œâ”€â”€ backend/                    # Node.js/Express Backend
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ Tweet.js           # MongoDB schema
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ auth.js            # Authentication routes
â”‚   â”‚   â””â”€â”€ tweets.js          # Tweet CRUD routes
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ twitterService.js  # Twitter API wrapper (for web frontend)
â”‚   â”œâ”€â”€ server.js              # Express server
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ frontend/                   # React Frontend (Optional)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js             # Main React component
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ Dashboard/                  # Python Desktop Dashboard â­ MODEL RUNS HERE
â”‚   â”œâ”€â”€ dashboard.py           # Main dashboard UI
â”‚   â”œâ”€â”€ model_inference.py     # Model wrapper (loads DeLTran15)
â”‚   â”œâ”€â”€ api_client.py          # Backend API client
â”‚   â”œâ”€â”€ token_highlighter.py   # Token highlighting logic
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ Trained_Model/             # Your trained model
â”‚   â”œâ”€â”€ deltran15_minilm_fp32.pt  # Model weights
â”‚   â”œâ”€â”€ Model.py               # Model architecture
â”‚   â”œâ”€â”€ Explainable_AI.py      # XAI implementation
â”‚   â”œâ”€â”€ Actionable_Info.py     # Actionable info extraction
â”‚   â””â”€â”€ Model_Tokenizer/       # Tokenizer files
â”‚
â”œâ”€â”€ Data_Set/                  # Training data
â”‚   â”œâ”€â”€ Unprocessed_Data_Sets/ # Raw disaster datasets
â”‚   â”œâ”€â”€ Processed_Data_Set/    # Preprocessed CSVs
â”‚   â””â”€â”€ Data_Preprocessing/   # Data prep scripts
â”‚
â”œâ”€â”€ Model_Build/               # Model training scripts
â”‚   â””â”€â”€ Build.py
â”‚
â”œâ”€â”€ run_dashboard.py           # Entry point for desktop dashboard
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** (v16+) and npm
- **Python** 3.8+
- **MongoDB** (local or MongoDB Atlas)

### 1. Backend Setup

```bash
cd backend
npm install
cp .env.example .env
# Edit .env with your MongoDB URI
npm start
```

Backend runs on `http://localhost:5000`

### 2. Desktop Dashboard Setup

```bash
# Install Python dependencies
pip install -r requirements.txt

# Download spaCy model (optional but recommended)
python -m spacy download en_core_web_sm

# Run dashboard
python run_dashboard.py
```

### 3. Frontend Setup (Optional)

```bash
cd frontend
npm install
npm start
```

Frontend runs on `http://localhost:3000`

## ğŸ¯ Classification Categories

The model classifies tweets into 5 categories:

1. **Affected Individuals** ğŸ”´ - People affected by disasters
2. **Infrastructure Damage** ğŸŸ  - Damaged infrastructure
3. **Not Humanitarian** âšª - Non-relevant tweets
4. **Other Information** ğŸ”µ - Other relevant information
5. **Rescue/Donation** ğŸŸ¢ - Rescue efforts or donations

## ğŸ’» Usage

### Desktop Dashboard (Primary Tool)

1. **Start backend**: `cd backend && npm start`
2. **Run dashboard**: `python run_dashboard.py`
3. **Dashboard connects** to backend server automatically
4. **Two ways to get tweets**:
   - **Manual Input**: Enter tweets directly in the dashboard
   - **From Database**: Load tweets already stored in MongoDB
5. **Tweets are automatically classified locally** using your model
6. **View results** with:
   - Color-coded category labels
   - Token-level highlighting (XAI)
   - Actionable information extraction
   - Filter by category

### Web Frontend (Optional)

1. **Start backend**: `cd backend && npm start`
2. **Start frontend**: `cd frontend && npm start`
3. **Login** with Twitter API credentials (for fetching tweets)
4. **Fetch tweets** from Twitter (stored in MongoDB)
5. **View tweets** in web interface (classification happens in desktop app)

## ğŸ“Š Data Flow

```
1. Get Tweets (Two Methods):
   a) Manual Input: Dashboard â†’ User enters tweet â†’ Saved to MongoDB
   b) From Database: Dashboard â†’ Backend â†’ MongoDB â†’ Load tweets

2. Classification (Desktop Dashboard Only):
   Dashboard â†’ Loads DeLTran15 model locally â†’ Classifies tweets
   â†’ Generates XAI explanations â†’ Extracts actionable info
   â†’ Saves classification results to MongoDB

3. View Results:
   Dashboard â†’ Backend â†’ MongoDB â†’ Display classified tweets
   â†’ Filter by category â†’ View details with token highlighting
```

## ğŸ”¬ Model Details

### Architecture

- **Base Model**: `sentence-transformers/all-MiniLM-L6-v2`
- **Custom Head**: Linear classifier on `[CLS]` embedding with dropout
- **Weights**: `Trained_Model/deltran15_minilm_fp32.pt`
- **Classes**: 5 disaster-related categories

### Explainable AI (XAI)

- Token-level importance scores using gradient-based methods
- Visual highlighting with color gradients (white â†’ red)
- Shows which tokens are most influential for classification

### Actionable Information Extraction

For actionable categories, the system extracts:
- ğŸ“ **Locations** - Geographic locations (via spaCy NER)
- ğŸ‘¥ **People Counts** - Number of affected people
- ğŸ†˜ **Needs** - Required resources (food, water, medicine, etc.)
- ğŸ’¥ **Damage Types** - Types of damage mentioned
- â° **Time Mentions** - Temporal information

## ğŸ”’ Privacy & Security

- âœ… **Model runs locally** - Never leaves your machine
- âœ… **No cloud classification** - All inference happens on your computer
- âœ… **Privacy-first** - Your data stays private
- âœ… **Offline capable** - Can work without internet (after fetching tweets)
- âœ… **No Twitter API in dashboard** - Works with database only

## ğŸ“š Documentation

- **[SETUP_GUIDE.md](SETUP_GUIDE.md)** - Detailed setup instructions
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Architecture overview
- **[Dashboard/README.md](Dashboard/README.md)** - Dashboard documentation
- **[README_MERN.md](README_MERN.md)** - MERN stack details

## ğŸ› ï¸ Development

### Training the Model

```bash
cd Model_Build
python Build.py
```

### Data Preprocessing

```bash
cd Data_Set/Data_Preprocessing
python Data_Extraction.py
python Data_Cleaning.py
```

### Running CLI Classifier

```bash
python Trained_Model/Main.py
```

## ğŸ“¦ Dependencies

### Python (Desktop Dashboard)
- `torch` - Deep learning framework
- `transformers` - Hugging Face transformers
- `customtkinter` - Modern UI framework
- `spacy` - NLP for actionable info extraction
- `requests` - HTTP client for backend API

### Node.js (Backend)
- `express` - Web framework
- `mongoose` - MongoDB ODM
- `twitter-api-v2` - Twitter API wrapper (for web frontend)
- `cors` - CORS middleware

### React (Frontend)
- `react` - UI framework
- `@mui/material` - Component library
- `axios` - HTTP client

## ğŸ› Troubleshooting

- **Model not loading**: Ensure `Trained_Model/deltran15_minilm_fp32.pt` exists
- **Backend errors**: Check MongoDB is running and `.env` is configured
- **Dashboard connection errors**: Ensure backend is running on port 5000
- **Import errors**: Install all dependencies from `requirements.txt`

## ğŸ“ Notes

- The model weights (`deltran15_minilm_fp32.pt`) are large files
- Classification happens **only** in the desktop dashboard
- Backend stores raw tweets and receives classification results
- Dashboard works with database tweets only (no Twitter API needed)
- Frontend is optional - dashboard can work standalone

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- Fine-tuned DeLTran15 model for disaster classification
- Twitter API for tweet data (via web frontend)
- Hugging Face for transformer models
